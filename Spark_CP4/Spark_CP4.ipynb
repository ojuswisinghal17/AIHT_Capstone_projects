{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1:** Explore RDD in spark"
      ],
      "metadata": {
        "id": "t-UP7aeXdjBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction:**\n",
        "Apache Spark is a powerful distributed computing framework known for its speed and ease of use. At the heart of Spark lies the Resilient Distributed Dataset (RDD), a fundamental abstraction that represents distributed data collections across the Spark cluster. RDDs provide fault-tolerant, parallelized operations on data, enabling efficient processing of large-scale datasets."
      ],
      "metadata": {
        "id": "MEqQc1-9iqTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD (Resilient Distributed Dataset):**\n",
        "\n",
        "**Definition and Characteristics:** RDD is a fundamental abstraction in Apache Spark, representing a distributed collection of elements that can be operated on in parallel.\n",
        "\n",
        "Key characteristics include immutability, fault tolerance, and laziness.\n",
        "\n",
        "**Immutability:** Once created, RDDs cannot be changed. However, you can apply transformations to derive new RDDs.\n",
        "\n",
        "**Resilience to Failures and Lineage Graph:** RDDs are resilient to failures due to their lineage graph, which tracks the sequence of transformations applied to the base dataset. In case of data loss or failure, Spark can recompute lost partitions by tracing back the lineage graph and reapplying transformations from the original dataset.\n",
        "\n",
        "**Lazy Evaluation:** Transformations on RDDs are lazily evaluated, meaning they are not executed immediately but remembered for future execution. This allows Spark to optimize the execution plan by combining multiple transformations and executing them in a single pass."
      ],
      "metadata": {
        "id": "pkb7bAIeoc-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD Operations:**\n",
        "\n",
        "**Transformation Operations:** Transformation operations create a new RDD from an existing RDD by applying a function to each element of the RDD.\n",
        "\n",
        "Examples include:\n",
        "\n",
        "*   map(func): Applies a function to each element and returns a new RDD.\n",
        "*   filter(func): Filters elements based on a predicate function.\n",
        "*   flatMap(func): Similar to map, but each input item can be mapped to zero or more output items.\n",
        "\n",
        "These operations are lazily evaluated and do not trigger execution until an action operation is called.\n",
        "\n",
        "**Action Operations:** Action operations are operations that trigger the execution of transformations and return results to the driver program or write data to external storage systems.\n",
        "\n",
        "Examples include:\n",
        "*   reduce(func): Aggregate the elements of the RDD using a specified function.\n",
        "*   collect(): Return all elements of the RDD to the driver program.\n",
        "*   count(): Return the number of elements in the RDD.\n",
        "*   take(n): Return the first n elements of the RDD.\n",
        "\n",
        "Action operations are eagerly evaluated and cause Spark to execute the previously defined transformations.\n",
        "\n",
        "**Narrow vs. Wide Transformations:**\n",
        "\n",
        "**Narrow Transformations:** Transformations where each input partition contributes to only one output partition. Examples include map, filter, etc. These transformations can be computed in parallel without shuffling data across partitions.\n",
        "\n",
        "**Wide Transformations:** Transformations where each input partition contributes to multiple output partitions. Examples include groupByKey, reduceByKey, sortByKey, etc. These transformations may require shuffling data across partitions, leading to increased network I/O and reduced performance."
      ],
      "metadata": {
        "id": "i9ysZFCyjl2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD Persistence:**\n",
        "\n",
        "**Caching and Persistence Levels:**\n",
        "\n",
        "RDD persistence refers to storing the contents of an RDD in memory or disk for reuse.\n",
        "Spark supports different persistence levels, including MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, etc.\n",
        "\n",
        "**Caching:** Caching RDDs in memory can significantly improve performance by avoiding recomputation of costly transformations.\n",
        "\n",
        "**Benefits of Caching RDDs:** Caching is beneficial when the same RDD is used multiple times, the RDD is costly to compute, or iterative algorithms are applied to the RDD."
      ],
      "metadata": {
        "id": "FYtRwptOo0Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partitioning in RDD:**\n",
        "\n",
        "**Concept of Partitions:**\n",
        "\n",
        "Partitions are the basic units of parallelism in Spark, representing subsets of the RDD's data distributed across the cluster.\n",
        "RDDs are divided into partitions to enable parallel processing.\n",
        "Spark operations are performed independently on each partition, allowing for efficient distributed processing.\n",
        "\n",
        "**Custom Partitioning Strategies:**\n",
        "\n",
        "Spark allows custom partitioning strategies to control how data is distributed across partitions.\n",
        "Users can define custom partitioners based on specific criteria, such as key-based partitioning for groupByKey or reduceByKey operations.\n",
        "Custom partitioning strategies can optimize data locality and improve performance for certain types of operations."
      ],
      "metadata": {
        "id": "Yz38aibor4Ij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RDD Lineage:**\n",
        "\n",
        "**Understanding RDD Lineage:**\n",
        "\n",
        "RDD lineage refers to the sequence of transformations applied to an RDD to derive a new RDD.\n",
        "Spark maintains the lineage graph, which records the dependencies between RDDs and the sequence of transformations applied.\n",
        "Lineage information allows Spark to recover from failures by reapplying transformations from the original dataset.\n",
        "\n",
        "**Importance in Fault Tolerance and Recovery:**\n",
        "\n",
        "RDD lineage is crucial for fault tolerance and recovery in Spark.\n",
        "In case of data loss or failure, Spark can recompute lost partitions by tracing back the lineage graph and reapplying transformations from the original dataset.\n",
        "Lineage information ensures that Spark applications can recover from failures without data loss, making them resilient to faults."
      ],
      "metadata": {
        "id": "vMupWGrIsMM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A simple code implementation demonstrating the concepts of RDDs in Apache Spark using Python and PySpark:"
      ],
      "metadata": {
        "id": "xAQzYXcYsseh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"RDDExample\")\n",
        "\n",
        "# Create an RDD from a list\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Perform transformation operations\n",
        "squared_rdd = rdd.map(lambda x: x*x)\n",
        "filtered_rdd = squared_rdd.filter(lambda x: x > 10)\n",
        "\n",
        "# Perform action operations\n",
        "result = filtered_rdd.collect()\n",
        "count = filtered_rdd.count()\n",
        "\n",
        "# Output the result\n",
        "print(\"Filtered elements:\", result)\n",
        "print(\"Count of filtered elements:\", count)\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9CEcOKTit0R",
        "outputId": "3279b53a-90e3-49e3-8d27-42fcb245d540"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered elements: [16, 25]\n",
            "Count of filtered elements: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A PySpark code to find the sum of squares of numbers greater than 10 from a given list using RDD operations:"
      ],
      "metadata": {
        "id": "ytSpIEA1s67X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"RDDExample\")\n",
        "\n",
        "# Create an RDD from a list\n",
        "data = [1, 2, 3, 4, 5, 11, 12, 13]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Filter numbers greater than 10 and calculate sum of squares\n",
        "filtered_rdd = rdd.filter(lambda x: x > 10)\n",
        "squared_rdd = filtered_rdd.map(lambda x: x*x)\n",
        "sum_of_squares = squared_rdd.reduce(lambda x, y: x + y)\n",
        "\n",
        "# Output the result\n",
        "print(\"Sum of squares of numbers greater than 10:\", sum_of_squares)\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZnUojzWtFbK",
        "outputId": "a7ffd450-f3b0-4026-f1e9-68039c43625d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of squares of numbers greater than 10: 434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2:** In PySpark, create a program that reads a CSV file containing sales data, performs data cleaning by handling missing values and removing duplicates, calculates the total sales amount for each product, and finally, outputs the results to a new CSV file. Ensure to use transformations and actions in your PySpark script"
      ],
      "metadata": {
        "id": "FtKRy_OxX2z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE0Zzb3mZq0f",
        "outputId": "2025ab62-2367-4161-93a7-72c4c7c957f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=cd7507e18bb9572da97e3fce1f37d4ffdbaaf1004e302d5ed695c6e8144fdcc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM3rL3l0WgxG",
        "outputId": "8a5e70d2-dd3b-4cd9-ab9b-e02c799fc42a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Transaction ID: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Customer ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Product Category: string (nullable = true)\n",
            " |-- Quantity: string (nullable = true)\n",
            " |-- Price per Unit: string (nullable = true)\n",
            " |-- Total Amount: string (nullable = true)\n",
            "\n",
            "+----------------+-----------+\n",
            "|Product Category|Total Sales|\n",
            "+----------------+-----------+\n",
            "|     Electronics|   156905.0|\n",
            "|        Clothing|   155580.0|\n",
            "|          Beauty|   143515.0|\n",
            "+----------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, when, count, desc\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "sales_data = spark.read.option(\"header\", \"true\").csv(\"/sales_data.csv\")\n",
        "\n",
        "# Print the schema of the DataFrame\n",
        "sales_data.printSchema()\n",
        "\n",
        "# Handle missing values\n",
        "cleaned_data = sales_data.na.drop()\n",
        "\n",
        "# Remove duplicates\n",
        "distinct_data = cleaned_data.dropDuplicates()\n",
        "\n",
        "# Calculate the total sales amount for each product\n",
        "result = distinct_data.groupBy(\"Product Category\") \\\n",
        "                      .agg(sum(\"Total Amount\").alias(\"Total Sales\")) \\\n",
        "                      .sort(desc(\"Total Sales\"))\n",
        "\n",
        "# Show the result\n",
        "result.show()\n",
        "\n",
        "# Write the result to a new CSV file\n",
        "result.coalesce(1).write.option(\"header\", \"true\").csv(\"/output.csv\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    }
  ]
}